{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db41bb2b-0379-40e1-92e0-1e0445bafda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yufeiquan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yufeiquan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       This position is based at our Luxembourg headq...\n",
       "1       Background\\nDiversity, Equity and Inclusion ar...\n",
       "2       Job Title: Planview Administrator / Analyst\\nL...\n",
       "3       Location – Fountain Valley, CA\\nPurpose\\nHAEA ...\n",
       "4       Business Process Analyst I\\nPURPOSE\\nAnalyze, ...\n",
       "                              ...                        \n",
       "2778    Basic Qualifications\\n-  Experience in process...\n",
       "2779    We are seeking a passionate and experienced Wo...\n",
       "2780    We are seeking a skilled and technically profi...\n",
       "2781    - Job Title: Business Professional - Marketing...\n",
       "2782    An innovative technology company is hiring for...\n",
       "Name: job_description, Length: 2783, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('linkedin_jobs.csv')  # Replace with your file name\n",
    "text_data = df['job_description']  # Replace with the column name containing job descriptions\n",
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775edd96-d281-4406-b8c3-2d4b1d411ebb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47920a3d-009e-483d-b27f-8b5bffa2a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_variations = {\n",
    "    # Programming and Tools\n",
    "    \"python\": [\"python\", \"py\"],\n",
    "    \"r\": [\"r\", \"r programming\"],\n",
    "    \"sql\": [\"sql\", \"sql server\", \"nosql\", \"mysql\", \"postgresql\", \"oracle\", \"sql databases\", \"structured query language\"],\n",
    "    \"vba\": [\"vba\", \"visual basic for applications\"],\n",
    "    \"matlab\": [\"matlab\", \"matrix laboratory\"],\n",
    "    \"programming\": [\"programming\", \"coding\", \"software development\", \"application development\"],\n",
    "    \"java\": [\"java\", \"j2ee\"],\n",
    "    \"javascript\": [\"javascript\", \"js\", \"node.js\", \"react.js\", \"angular.js\", \"vue.js\"],\n",
    "    \"c++\": [\"c++\", \"cpp\"],\n",
    "    \"c#\": [\"c#\", \"c sharp\", \".net\", \"dot net\"],\n",
    "    \"excel\": [\"excel\", \"advanced excel\", \"google sheets\", \"spreadsheet analysis\"],\n",
    "    \"data visualization\": [\"data visualization\", \"visualization\", \"tableau\", \"power bi\", \"matplotlib\", \"seaborn\", \"ggplot\", \"dash\", \"plotly\"],\n",
    "    \"etl\": [\"etl\", \"data pipelines\", \"data integration\", \"extract transform load\"],\n",
    "    \"google big query\": [\"google big query\", \"big query\"],\n",
    "\n",
    "    # Cloud Platforms and Computing\n",
    "    \"cloud computing\": [\"cloud computing\", \"cloud\", \"aws\", \"azure\", \"google cloud\", \"gcp\"],\n",
    "    \"aws\": [\"aws\", \"amazon web services\"],\n",
    "    \"azure\": [\"azure\", \"microsoft azure\"],\n",
    "    \"google cloud\": [\"google cloud\", \"gcp\", \"google cloud platform\"],\n",
    "    \"adf\": [\"adf\", \"azure data factory\"],\n",
    "    \"databricks\": [\"databricks\"],\n",
    "    \"hadoop\": [\"hadoop\", \"apache hadoop\"],\n",
    "    \"spark\": [\"spark\", \"apache spark\"],\n",
    "    \"snowflake\": [\"snowflake\"],\n",
    "    \"redshift\": [\"redshift\"],\n",
    "    \"big data\": [\"big data\", \"data lakes\"],\n",
    "\n",
    "    # Machine Learning and AI\n",
    "    \"machine learning\": [\"machine learning\", \"ml\", \"ml algorithms\", \"automated learning\"],\n",
    "    \"tensorflow\": [\"tensorflow\", \"tf\"],\n",
    "    \"pytorch\": [\"pytorch\", \"torch\"],\n",
    "    \"scikit-learn\": [\"scikit-learn\", \"sklearn\", \"scikit learn\"],\n",
    "    \"deep learning\": [\"deep learning\", \"neural networks\", \"dl\"],\n",
    "    \"artificial intelligence\": [\"artificial intelligence\", \"ai\", \"ai systems\"],\n",
    "    \"nlp\": [\"natural language processing\", \"nlp\", \"text mining\", \"text analytics\", \"language models\"],\n",
    "    \"mlops\": [\"mlops\", \"machine learning operations\", \"model deployment\", \"model monitoring\"],\n",
    "\n",
    "    # Analytics and Reporting\n",
    "    \"data analysis\": [\"data analysis\", \"data-driven decision making\", \"collecting and cleaning datasets\"],\n",
    "    \"financial analysis\": [\"financial analysis\", \"financial forecasting\", \"business analysis\"],\n",
    "    \"statistical analysis\": [\"statistical analysis\", \"statistics\", \"statistical modeling\"],\n",
    "    \"business analysis\": [\"business analysis\", \"business requirements analysis\"],\n",
    "    \"pricing analysis\": [\"pricing analysis\", \"pricing models\", \"dynamic pricing\", \"margin optimization\"],\n",
    "    \"data modeling\": [\"data modeling\", \"semantic layer\"],\n",
    "    \"ad hoc analysis\": [\"ad hoc analysis\"],\n",
    "    \"dashboards\": [\"dashboards\", \"dashboard development\"],\n",
    "    \"revenue management\": [\"revenue management\", \"pricing domains\", \"customer lifetime value analysis\"],\n",
    "    \"optimization\": [\"optimization\", \"margin optimization\", \"cost-saving initiatives\"],\n",
    "\n",
    "    # Responsibilities (Actionable Tasks)\n",
    "    \"collaboration\": [\n",
    "        \"collaborating with Data Architecture, Governance, and Business Teams\",\n",
    "        \"acting as a liaison between stakeholders and technical teams\",\n",
    "        \"partnering with internal teams, including sales and operations\",\n",
    "        \"facilitating cross-functional collaboration to identify trends and challenges\"\n",
    "    ],\n",
    "    \"data governance\": [\n",
    "        \"designing and implementing access controls\",\n",
    "        \"ensuring data accuracy and validation\",\n",
    "        \"establishing best practices for data governance and compliance\",\n",
    "        \"monitoring data security standards\"\n",
    "    ],\n",
    "    \"training and enablement\": [\n",
    "        \"developing training materials\",\n",
    "        \"promoting self-service capabilities\",\n",
    "        \"building data literacy resources\",\n",
    "        \"delivering tailored training sessions\"\n",
    "    ],\n",
    "    \"reporting\": [\n",
    "        \"creating financial reports\",\n",
    "        \"designing dashboards\",\n",
    "        \"maintaining and updating performance metrics\",\n",
    "        \"tracking and analyzing operational data\"\n",
    "    ],\n",
    "    \"forecasting\": [\n",
    "        \"building predictive models\",\n",
    "        \"conducting revenue forecasting\",\n",
    "        \"performing demand elasticity analysis\",\n",
    "        \"evaluating trends for pricing strategies\"\n",
    "    ],\n",
    "    \"investment analysis\": [\n",
    "        \"preparing capital investment data\",\n",
    "        \"supporting asset valuation\",\n",
    "        \"analyzing investment strategies\",\n",
    "        \"evaluating liquidity\"\n",
    "    ],\n",
    "    \"inventory management\": [\n",
    "        \"tracking and forecasting advertising and digital inventory\",\n",
    "        \"optimizing yield\",\n",
    "        \"forecasting inventory availability\",\n",
    "        \"resolving campaign or programming conflicts\"\n",
    "    ],\n",
    "    \"project management\": [\n",
    "        \"managing project workflows in Agile environments\",\n",
    "        \"documenting user stories, epics, and technical requirements\",\n",
    "        \"tracking tasks using JIRA\",\n",
    "        \"ensuring alignment with strategic goals\"\n",
    "    ],\n",
    "    \"communication\": [\n",
    "        \"presenting insights to stakeholders\",\n",
    "        \"translating technical results for non-technical audiences\",\n",
    "        \"preparing client-facing presentations\",\n",
    "        \"influencing leadership through data-driven recommendations\"\n",
    "    ],\n",
    "\n",
    "    # Tools and Technologies\n",
    "    \"sap\": [\"sap\"],\n",
    "    \"operative.one\": [\"operative.one\"],\n",
    "    \"freewheel\": [\"freewheel\"],\n",
    "    \"google ad manager\": [\"google ad manager\", \"gam\"],\n",
    "    \"nielsen dar\": [\"nielsen dar\"],\n",
    "\n",
    "    # Soft Skills and Learning\n",
    "    \"critical thinking\": [\"critical thinking\", \"analytical thinking\"],\n",
    "    \"continuous learning\": [\"continuous learning\", \"self-learning\", \"lifelong learning\"],\n",
    "    \"leadership\": [\"leadership\", \"mentoring\", \"team leadership\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a0e447-e885-472f-a892-5d19e5148f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yufeiquan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yufeiquan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 n-grams by TF-IDF score:\n",
      "data : 0.0621\n",
      "experience : 0.0555\n",
      "business : 0.0527\n",
      "financial : 0.0401\n",
      "work : 0.0398\n",
      "management : 0.0362\n",
      "skills : 0.0356\n",
      "team : 0.0342\n",
      "support : 0.0297\n",
      "ability : 0.0296\n",
      "analysis : 0.0291\n",
      "including : 0.0261\n",
      "research : 0.0255\n",
      "requirements : 0.0254\n",
      "systems : 0.0252\n",
      "role : 0.0251\n",
      "strong : 0.0246\n",
      "analyst : 0.0245\n",
      "risk : 0.0242\n",
      "investment : 0.0239\n",
      "job : 0.0238\n",
      "position : 0.0237\n",
      "company : 0.0237\n",
      "required : 0.0230\n",
      "benefits : 0.0228\n",
      "information : 0.0227\n",
      "development : 0.0224\n",
      "years : 0.0215\n",
      "process : 0.0210\n",
      "operations : 0.0210\n",
      "hr : 0.0209\n",
      "project : 0.0206\n",
      "knowledge : 0.0205\n",
      "solutions : 0.0205\n",
      "new : 0.0203\n",
      "processes : 0.0202\n",
      "teams : 0.0198\n",
      "related : 0.0196\n",
      "ensure : 0.0193\n",
      "marketing : 0.0192\n",
      "reporting : 0.0192\n",
      "working : 0.0191\n",
      "employee : 0.0190\n",
      "employees : 0.0188\n",
      "technical : 0.0185\n",
      "environment : 0.0184\n",
      "performance : 0.0184\n",
      "status : 0.0181\n",
      "client : 0.0181\n",
      "product : 0.0178\n",
      "\n",
      "Sample of extracted skills:\n",
      "                                     job_description  \\\n",
      "0  This position is based at our Luxembourg headq...   \n",
      "1  Background\\nDiversity, Equity and Inclusion ar...   \n",
      "2  Job Title: Planview Administrator / Analyst\\nL...   \n",
      "3  Location – Fountain Valley, CA\\nPurpose\\nHAEA ...   \n",
      "4  Business Process Analyst I\\nPURPOSE\\nAnalyze, ...   \n",
      "5  20240 - Web Business Analyst\\nSavannah, GA (on...   \n",
      "6  Date Posted:\\n2024-12-06\\nCountry:\\nUnited Sta...   \n",
      "7  Blackstone is the world’s largest alternative ...   \n",
      "8  Salary Range: 90,525.00-128,200.00\\nCompany De...   \n",
      "9  Location: \\n San Francisco\\nFirm Overview\\nFin...   \n",
      "\n",
      "                                          Key_Skills  \n",
      "0                              [python, programming]  \n",
      "1                       [excel, continuous learning]  \n",
      "2                                                 []  \n",
      "3  [leadership, financial analysis, business anal...  \n",
      "4  [data visualization, optimization, excel, fina...  \n",
      "5  [optimization, java, javascript, sap, programm...  \n",
      "6                   [excel, sap, financial analysis]  \n",
      "7                                            [excel]  \n",
      "8  [etl, pytorch, machine learning, deep learning...  \n",
      "9                               [financial analysis]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# If not already done:\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Read the CSV file\n",
    "# ------------------------------------------------------------------------------\n",
    "df = pd.read_csv('linkedin_jobs.csv')  # Replace with your file name\n",
    "df = df.dropna(subset=['job_description'])  # drop rows where job_description is NaN\n",
    "text_data = df['job_description']\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Preprocess the text\n",
    "# ------------------------------------------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords & keep only alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "df['Tokens'] = text_data.apply(preprocess_text)\n",
    "\n",
    "# Create a clean text column (for TF-IDF vectorizer)\n",
    "df['Cleaned_Text'] = df['Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Extract n-grams (up to trigrams) using TF-IDF\n",
    "# ------------------------------------------------------------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),    # capture unigrams, bigrams, trigrams\n",
    "    stop_words='english',  # additional removal of common English words\n",
    "    min_df=2,             # only consider n-grams that appear at least twice\n",
    "    max_features=3000      # limit the features (tune based on your dataset size)\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Cleaned_Text'])\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate average TF-IDF for each n-gram across all documents\n",
    "avg_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "feature_tfidf_pairs = list(zip(features, avg_tfidf))\n",
    "\n",
    "# Sort by descending TF-IDF\n",
    "feature_tfidf_pairs = sorted(feature_tfidf_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 50 n-grams by TF-IDF score:\")\n",
    "for feature, score in feature_tfidf_pairs[:50]:\n",
    "    print(f\"{feature} : {score:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. Dictionary-based skill matching with synonyms/variations\n",
    "#    (expand this dictionary as needed for your domain)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def extract_skills(token_list, skill_dict):\n",
    "    \"\"\"\n",
    "    token_list: list of preprocessed tokens from one job description\n",
    "    skill_dict: dict with key=canonical skill, value=list of synonyms/variations\n",
    "    Returns a list of matched canonical skills found in the text.\n",
    "    \"\"\"\n",
    "    found_skills = set()\n",
    "    # Convert token_list to lower for matching (already lower, but just in case)\n",
    "    tokens = [t.lower() for t in token_list]\n",
    "\n",
    "    for skill_canonical, variations in skill_dict.items():\n",
    "        for variant in variations:\n",
    "            variant_tokens = variant.lower().split()\n",
    "            if len(variant_tokens) == 1:\n",
    "                # Single-word check\n",
    "                if variant_tokens[0] in tokens:\n",
    "                    found_skills.add(skill_canonical)\n",
    "            else:\n",
    "                # Multi-word check (e.g., \"machine learning\")\n",
    "                # We'll look for consecutive matches in the token list\n",
    "                for i in range(len(tokens) - len(variant_tokens) + 1):\n",
    "                    if tokens[i:i+len(variant_tokens)] == variant_tokens:\n",
    "                        found_skills.add(skill_canonical)\n",
    "                        break\n",
    "\n",
    "    return list(found_skills)\n",
    "\n",
    "df['Key_Skills'] = df['Tokens'].apply(lambda x: extract_skills(x, skill_variations))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Inspect results\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nSample of extracted skills:\")\n",
    "print(df[['job_description', 'Key_Skills']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2181ce87-ab29-432d-a2a8-41218788c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of skills and count occurrences\n",
    "all_skills = [skill for skills in df['Key_Skills'] if isinstance(skills, list) for skill in skills]\n",
    "skill_counts = Counter(all_skills)\n",
    "\n",
    "# Convert to a DataFrame for better visualization\n",
    "skills_df = pd.DataFrame(skill_counts.items(), columns=['Skill', 'Count']).sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Save or inspect the full DataFrame as needed\n",
    "skills_df.to_csv('skill_counts.csv', index=False)  # Optional: Save to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb879240-83dd-4e26-8dd8-d685f9924657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skill</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>excel</td>\n",
       "      <td>1072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>leadership</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sql</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data visualization</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>data analysis</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>programming</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>python</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>statistical analysis</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>financial analysis</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dashboards</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cloud computing</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>optimization</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>r</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>machine learning</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>critical thinking</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business analysis</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sap</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>etl</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>azure</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>aws</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>continuous learning</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>nlp</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>data modeling</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>java</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>big data</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>vba</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tensorflow</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>deep learning</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>snowflake</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>spark</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>google cloud</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>javascript</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>hadoop</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>databricks</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>matlab</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ad hoc analysis</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>pricing analysis</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mlops</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>revenue management</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>redshift</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>adf</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>forecasting</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>google big query</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>reporting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>training and enablement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>google ad manager</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>freewheel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>c#</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Skill  Count\n",
       "2                     excel   1072\n",
       "4                leadership    754\n",
       "12                      sql    671\n",
       "8        data visualization    529\n",
       "22            data analysis    529\n",
       "1               programming    478\n",
       "0                    python    426\n",
       "18     statistical analysis    388\n",
       "5        financial analysis    363\n",
       "26               dashboards    322\n",
       "19          cloud computing    267\n",
       "7              optimization    255\n",
       "28                        r    205\n",
       "15         machine learning    203\n",
       "17  artificial intelligence    194\n",
       "23        critical thinking    181\n",
       "6         business analysis    159\n",
       "11                      sap    129\n",
       "13                      etl    116\n",
       "25                    azure    101\n",
       "21                      aws     91\n",
       "3       continuous learning     76\n",
       "24                      nlp     70\n",
       "38            data modeling     65\n",
       "9                      java     62\n",
       "30                 big data     52\n",
       "40                      vba     51\n",
       "14                  pytorch     49\n",
       "20               tensorflow     48\n",
       "16            deep learning     48\n",
       "39                snowflake     46\n",
       "35                    spark     44\n",
       "29             google cloud     42\n",
       "10               javascript     39\n",
       "34                   hadoop     31\n",
       "37               databricks     27\n",
       "31                   matlab     24\n",
       "32          ad hoc analysis     18\n",
       "36         pricing analysis     18\n",
       "27                    mlops     13\n",
       "33       revenue management      8\n",
       "42                 redshift      7\n",
       "41                      adf      5\n",
       "44              forecasting      4\n",
       "43         google big query      3\n",
       "45                reporting      2\n",
       "46  training and enablement      1\n",
       "47        google ad manager      1\n",
       "48                freewheel      1\n",
       "49             scikit-learn      1\n",
       "50                       c#      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60a3b1-aaee-4963-b2f4-9bad4d853fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
